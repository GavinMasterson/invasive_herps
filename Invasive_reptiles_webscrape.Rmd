---
title: "Invasive Herpetofauna"
author: "Gavin Masterson"
date: "29/11/2019"
output: html_document
---

```{r global options, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Invasive Herpetofauna

How many species of alien invasive herpetofauna are there globally? What are they? Where are they from? Where are they invasive? Keeping up to date with global trends and situations in herpetofaunal conservation is an ongoing challenge. Some of the most infamous invasive species are reptiles - think of the Burmese Python (*Python bivittatus*) in Florida, or the Brown Tree Snake (*Boiga irregularis*) in Guam - and amphibians - think of the Cane Toad (*Rhinella marina*) in Australia or the Common Platanna (*Xenopus laevis*) in Europe. As a herpetologist, I wanted to develop a reproducible webscraping method that will allow me to keep up to date on alien/invasive herpetofaunal species. 

[The Global Invasive Species Database](http://www.iucngisd.org/gisd/) (GISD) is a product of the work of the IUCN's Species Survival Commission. Initial development of the GISD took place between 1998 and 2000, and the database received a functionality/cosmetic upgrade in 2004. The GISD database is an ongoing project and is curated by the work of specialists who volunteer their valuable time and expertise. More information can be found [here](http://www.iucngisd.org/gisd/about.php) on the GISD website.

For the work presented below, I was inspired by [this tutorial](https://naturaldatasolutions.com/2018/11/28/scraping-and-visualizing-the-gisd-with-r/) by Jim Sheehan on November 28, 2018. Jim's work inspired me to do this, my first webscraping project, and I relied on his post for navigating the mechanics of the coding. I am grateful to him for his post.

```{r setup, include = TRUE}
library(robotstxt)  # check website scraping rules
library(readr)  # reading data
library(dplyr)  # "wrangling" data
library(stringr) # manipulating strings
library(rvest)  # web scraping
library(knitr)  # HTML table creation
library(kableExtra)  # additional HTML table formatting
library(rworldmap)  # world chloropleth mapping
```

## First 10 rows of Invasive Herpetofauna

```{r import, include = TRUE}
sp_list <- read.csv2("amrep_gisd.csv", strip.white = TRUE, stringsAsFactors = FALSE) #Not sure why it adds the 8th column with NA values
sp_list[2:15,5] <- "Squamata"
sp_list <- sp_list[,-8]
              
kable(sp_list[1:10,]) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

## Webscraping Invasive Herpetofauna from the GISD

Herpetofaunal taxonomy  because of the historical use of subspecies.  This means that we need to allow for the possibility of species whose names contain a *generic name*, *specific name* and a *subspecific name* in the "Species" column of our dataset. For example, the Red-eared Slider is a widespread invasive that is known by the Latin name of *Trachemys scripta elegans*, where the subspecific name is "elegans". The problem here is that whenever a species has no *subspecific name*, the code below will add "+NA" to the end of the URL rendering it useless for accessing the species page.

```{r URLs}

sp_list <- sp_list %>% 
  tidyr::separate(Species, c("genus", "species", "subspecies"), sep = " ", remove = FALSE) %>% 
  tidyr::unite(c(genus, species, subspecies), col="genspp", sep = "+", remove = FALSE) %>% 
  mutate(url=paste0("http://www.iucngisd.org/gisd/speciesname/", genspp)) %>% 
  select(-genspp)

sp_list[sample(nrow(sp_list),5),]$url  # display a random sample of 5 URLs
```

To fix the URLs we need to snip the ends of the URLS that end in exactly "+NA". The code below has the additional step for removing "+NA" from the URLs of species that do not have a subspecific name. (Note: if you are using R in a language other than English, you should refer to documentation for the 'locale'argument of the 'coll()' function in stringr.)

```{r fixed URLs, include = TRUE}

sp_list$url <- str_replace(sp_list$url, pattern = coll(pattern = "+NA"), replacement = "")

sp_list[c(1,15,17, 19, 40),]$url  # display a random sample of 5 URLs
```

The URLs are now correctly specified and we can now scrape each of the species reports. As metnioned above, I'm interested in keeping up-to-date with the total number of invasive herpetofauna, where they're invasive and where they're from. The total number can already be deduced from the length of the sp_list object. Now we need to scrape the lists of countries where each species is native and where they have established. Below I use the same approach that Jim Sheehan used in the post I linked above. It involves looping across each URL in the sp_list object and selecting the correct HTML object to populate the two, pre-specified lists that are created to receive the scraped information. I have modified the object names, have not used 'tryCatch' and I have directly assigned each scrape into the relevant list.

```{r scrape}
urls <- pull(sp_list, url)
sp_names <- pull(sp_list, Species)
a_range <- vector(mode = "list", length = length(urls))
n_range <- vector(mode = "list", length = length(urls))

names(a_range) <- sp_names; names(n_range) <- sp_names

# Scrape each URL with a 5-second delay in between each iteration

for(i in 1:length(urls)){
    a_range[[i]] <-  read_html(urls[i]) %>% 
                html_nodes(xpath = '//*[@id="l-1st-step"]') %>% 
                html_nodes("li") %>% 
                html_text()
    
    n_range[[i]] <-  read_html(urls[i]) %>% 
                html_nodes(xpath = '//*[@id="nr-col"]') %>% 
                html_nodes("li") %>% 
                html_text()
    
    Sys.sleep(5)
}
rm(i)
```


# Number of alien range countries for each species

# Inferred date of establishment?



